{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.sentences('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.nouns('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.pos('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "hannanum.nouns('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum.morphs('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum.pos('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.nouns('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.morphs('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.pos('한국어 분석을 시작합니다 재미있어요~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('./data/09. alice.txt').read()\n",
    "alice_mask = np.array(Image.open('./data/09. alice_mask.png'))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"said\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "path=\"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == \"Darwin\":\n",
    "    rc('font', family='AppleGothic')\n",
    "\n",
    "elif platform.system() == \"Windows\":\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()    \n",
    "    rc('font', family=font_name)\n",
    "\n",
    "else:\n",
    "    print('Unknown system... sorry')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(alice_mask, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', max_words=2000, mask=alice_mask, stopwords=stopwords)\n",
    "wc = wc.generate(text)\n",
    "wc.words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('./data/09. a_new_hope.txt').read()\n",
    "\n",
    "text = text.replace('HAN', 'Han')\n",
    "text = text.replace(\"LUKE'S\", 'Luke')\n",
    "\n",
    "mask = np.array(Image.open('./data/09. stormtrooper_mask.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('int')\n",
    "stopwords.add('ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(max_words=1000, mask=mask, stopwords=stopwords,\n",
    "               margin=10, random_state=1).generate(text)\n",
    "default_colors = wc.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return 'hsl(0, 0%%, %d%%)' % random.randint(60, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3), interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "files_ko = kobill.fileids()\n",
    "doc_ko = kobill.open('1809890.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt; t = Okt()\n",
    "tokens_ko = t.nouns(doc_ko)\n",
    "tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko, name='대한민국 국회 의안 제 1809890호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ko.tokens))\n",
    "print(len(set(ko.tokens)))\n",
    "ko.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "ko.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['.', '(', ')', ',', \"'\", '%', '-', 'X', ').', '×','의','자','에','안','번',\n",
    "                      '호','을','이','다','만','로','가','를']\n",
    "\n",
    "ko = [each_word for each_word in ko if each_word not in stop_words]\n",
    "ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(ko, name='대한민국 국회 의안 제 1809890호')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ko.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko.count('초등학교')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "ko.dispersion_plot(['육아휴직','초등학교','공무원'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko.concordance('초등학교')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ko.vocab().most_common(150)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    wordcloud = WordCloud(font_path='/Library/Fonts/AppleGothic.ttf',\n",
    "                      relative_scaling = 0.2,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "\n",
    "elif platform.system() == \"Windows\":\n",
    "    wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.2,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "\n",
    "else:\n",
    "    print('Unknown system... sorry')\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [('i like you', 'pos'),\n",
    "         ('i hate you', 'neg'),\n",
    "         ('you like me', 'neg'),\n",
    "         ('i like her', 'pos')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(word.lower() for sentence in train\n",
    "               for word in word_tokenize(sentence[0]))\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'i like MeRui'\n",
    "test_sent_features = {word.lower():\n",
    "                      (word in word_tokenize(test_sentence.lower()))\n",
    "                       for word in all_words}\n",
    "test_sent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [('메리가 좋아', 'pos'), \n",
    "         ('고양이도 좋아', 'pos'),\n",
    "         ('난 수업이 지루해', 'neg'),\n",
    "         ('메리는 이쁜 고양이야', 'pos'),\n",
    "         ('난 마치고 메리랑 놀거야', 'pos')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(word.lower() for sentence in train\n",
    "               for word in word_tokenize(sentence[0]))\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '난 수업이 마치면 메리랑 놀꺼야'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_features = {word.lower():\n",
    "                      (word in word_tokenize(test_sentence.lower()))\n",
    "                       for word in all_words}\n",
    "test_sent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [(tokenize(row[0]), row[1]) for row in train]\n",
    "train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_exists(doc):\n",
    "    return {word: (word in set(doc)) for word in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xy = [(term_exists(d), c) for d,c in train_docs]\n",
    "train_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [(\"난 수업이 마치면 메리랑 놀거야\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = pos_tagger.pos(test_sentence[0])\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_feature = { word: (word in tokens) for word in test_docs}\n",
    "test_sent_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = ['메리랑 놀러가고 싶지만 바쁜데 어떻하죠?',\n",
    "                   '메리는 공원에서 산책하고 노는 것을 싫어해요',\n",
    "                   '메리는 공원에서 노는 것도 싫어해요. 이상해요.',\n",
    "                   '먼 곳으로 여행을 떠나고 싶은데 너무 바빠서 그러질 못하고 있어요']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(contents)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_tokens = [t.morphs(row) for row in contents]\n",
    "contents_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_for_vectorize = []\n",
    "\n",
    "for content in contents_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "    contents_for_vectorize.append(sentence)\n",
    "\n",
    "contents_for_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(contents_for_vectorize)\n",
    "num_samples, num_features = X.shape\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = ['메리랑 공원에서 산책하고 놀고 싶어요']\n",
    "new_post_tokens = [t.morphs(row) for row in new_post]\n",
    "\n",
    "new_post_for_vectorize = []\n",
    "\n",
    "for content in new_post_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "    new_post_for_vectorize.append(sentence)\n",
    "\n",
    "new_post_for_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform(new_post_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    \n",
    "    print(\"== Post %i with dist=%.2f  : %s\" %(i,d,contents[i]))\n",
    "    \n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best post is %i, dist = %.2f\" % (best_i, best_dist))\n",
    "print('-->', new_post)\n",
    "print('---->', contents[best_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(contents)):\n",
    "    print(X.getrow(i).toarray())\n",
    "    \n",
    "print('------------------------')\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    \n",
    "    delta = v1_normalized - v2_normalized\n",
    "    \n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(\"== Post %i with dist=%.2f  : %s\" %(i,d,contents[i]))\n",
    "    \n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best post is %i, dist = %.2f\" % (best_i, best_dist))\n",
    "print('-->', new_post)\n",
    "print('---->', contents[best_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(t, d, D):\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
    "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, abb, abc = ['a'], ['a', 'b', 'b'], ['a', 'b', 'c']\n",
    "D = [a, abb, abc]\n",
    "\n",
    "print(tfidf('a', a, D))\n",
    "print(tfidf('b', abb, D))\n",
    "print(tfidf('a', abc, D))\n",
    "print(tfidf('b', abc, D))\n",
    "print(tfidf('c', abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1, decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_tokens = [t.morphs(row) for row in contents]\n",
    "\n",
    "contents_for_vectorize = []\n",
    "\n",
    "for content in contents_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "    contents_for_vectorize.append(sentence)\n",
    "\n",
    "X = vectorizer.fit_transform(contents_for_vectorize)\n",
    "num_samples, num_features = X.shape\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = ['근처 공원에 메리랑 놀러가고 싶네요']\n",
    "new_post_tokens = [t.morphs(row) for row in new_post]\n",
    "\n",
    "new_post_for_vectorize = []\n",
    "\n",
    "for content in new_post_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "    new_post_for_vectorize.append(sentence)\n",
    "\n",
    "new_post_for_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform(new_post_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(\"== Post %i with dist=%.2f  : %s\" %(i,d,contents[i]))\n",
    "    \n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "        \n",
    "print(\"Best post is %i, dist = %.2f\" % (best_i, best_dist))\n",
    "print('-->', new_post)\n",
    "print('---->', contents[best_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "path=\"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == \"Darwin\":\n",
    "    rc('font', family='AppleGothic')\n",
    "\n",
    "elif platform.system() == \"Windows\":\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()    \n",
    "    rc('font', family=font_name)\n",
    "\n",
    "else:\n",
    "    print('Unknown system... sorry')\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import urllib\n",
    "import urllib.request as urllib2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = 'http://search.naver.com/search.naver?query='\n",
    "tmp2 = '여친선물'\n",
    "\n",
    "tmp3 = ascii(tmp2)\n",
    "\n",
    "html = tmp1 + tmp3\n",
    "\n",
    "req = urllib2.Request(html)\n",
    "\n",
    "req.add_header('Referer', 'http://www.naver.com')\n",
    "\n",
    "req.add_header('User-Agent', 'Mozilla/5.0')\n",
    "response = urllib2.urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "tmp = soup.find_all('dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = []\n",
    "\n",
    "for line in tmp:\n",
    "    tmp_list.append(line.text)\n",
    "    \n",
    "tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp1 = 'http://search.naver.com/search.naver?query='\n",
    "key_word = 'girlfriendpresent'\n",
    "key_word_kr = key_word.encode('utf8')\n",
    "print(key_word_kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = 'https://search.naver.com/search.naver?where=kin'\n",
    "tmp2 = tmp1 + '&sm=tab_jum&ie=utf8&query=' + key_word\n",
    "tmp3 = tmp2 + '&start='\n",
    "\n",
    "print(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "present_candi_text = []\n",
    "\n",
    "for n in tqdm_notebook(range(1, 1000, 10)):\n",
    "    \n",
    "    html = tmp3 + str(n)\n",
    "    print(html)\n",
    "    req = urllib2.Request(html)\n",
    "\n",
    "    req.add_header('Referer', 'http://www.naver.com')\n",
    "\n",
    "    req.add_header('User-Agent', 'Mozilla/5.0')\n",
    "    \n",
    "    response = urllib2.urlopen(req)\n",
    "    \n",
    " #   response = urlopen(html.format(num=n, key_word=urllib.parse.quote('여자 친구 선물')))\n",
    "\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "    tmp = soup.find_all('dl')\n",
    "\n",
    "    for line in tmp:\n",
    "        present_candi_text.append(line.text)\n",
    "        \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from konlpy.tag import Okt; t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_text = ''\n",
    "\n",
    "for each_line in present_candi_text[:10000]:\n",
    "    present_text = present_text + each_line + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ko = t.morphs(present_text)\n",
    "tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko, name='여친선물')\n",
    "print(len(ko.tokens))\n",
    "print(len(set(ko.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko, name='여친선물')\n",
    "ko.vocab().most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['.','가','요','답변','...','을','수','에','질문','제','를','이','도',\n",
    "                      '좋','1','는','로','으로','2','것','은','다',',','니다','대','들',\n",
    "                      '2017','들','데','..','의','때','겠','고','게','네요','한','일','할',\n",
    "                      '10','?','하는','06','주','려고','인데','거','좀','는데','~','ㅎㅎ',\n",
    "                      '하나','이상','20','뭐','까','있는','잘','습니다','다면','했','주려',\n",
    "                      '지','있','못','후','중','줄','6','과','어떤','기본','!!',\n",
    "                      '단어','선물해','라고','중요한','합','가요','....','보이','네','무지']\n",
    "\n",
    "tokens_ko = [each_word for each_word in tokens_ko \n",
    "                                                         if each_word not in stop_words]\n",
    "\n",
    "ko = nltk.Text(tokens_ko, name='여자 친구 선물')\n",
    "ko.vocab().most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ko.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ko.vocab().most_common(300)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    wordcloud = WordCloud(font_path='/Library/Fonts/AppleGothic.ttf',\n",
    "                      relative_scaling = 0.5,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "\n",
    "elif platform.system() == \"Windows\":\n",
    "    wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.5,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "\n",
    "else:\n",
    "    print('Unknown system... sorry')\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open('./data/09. heart.jpg'))\n",
    "\n",
    "from wordcloud import ImageColorGenerator\n",
    "\n",
    "image_colors = ImageColorGenerator(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ko.vocab().most_common(200)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    wordcloud = WordCloud(font_path='/Library/Fonts/AppleGothic.ttf',\n",
    "                      relative_scaling = 0.1, mask=mask,\n",
    "                      min_font_size=1,\n",
    "                      max_font_size=100).generate_from_frequencies(dict(data))\n",
    "\n",
    "elif platform.system() == \"Windows\":\n",
    "    wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.1, mask=mask,\n",
    "                      background_color='white',\n",
    "                      min_font_size=1,\n",
    "                      max_font_size=100).generate_from_frequencies(dict(data))\n",
    "\n",
    "else:\n",
    "    print('Unknown system... sorry')\n",
    "\n",
    "default_colors = wordcloud.to_array()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from konlpy.tag import Okt; t = Okt()\n",
    "\n",
    "Okt = Okt()\n",
    "result = []\n",
    "lines = present_candi_text\n",
    "\n",
    "for line in lines:\n",
    "    malist = Okt.pos(line, norm=True, stem=True)\n",
    "    r = []\n",
    "    \n",
    "    for word in malist:\n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "    r1 = (\" \".join(r)).strip()\n",
    "    result.append(r1)\n",
    "    print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'pres_girl.data'\n",
    "with open(data_file, 'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = word2vec.LineSentence(data_file)\n",
    "model = word2vec.Word2Vec(data, size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "\n",
    "model.save('pres_girl.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load('pres_girl.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['문장'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['여친', '여자친구'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['시계', '목걸이'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['가방'], negative=['여친'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
